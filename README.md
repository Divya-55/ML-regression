ğŸ¥ Patient Stay Estimator â€“ Regression Model
ğŸ“˜ Project Overview
This project predicts how long a patient is likely to stay in the hospital, measured in days, using historical medical and demographic data.
By estimating the Length of Stay (LOS) before or during admission, hospitals can better manage beds, staff allocation, and resources â€” improving both efficiency and patient care.
________________________________________
ğŸ¯ Objective
Develop a Machine Learning Regression Model that accurately predicts the hospital stay duration (length_of_stay) based on various patient-level and clinical features such as demographics, comorbidities, and lab test results.
________________________________________
ğŸ“Š Dataset Details
Dataset: LengthOfStay.csv
Type: Healthcare (encounter-level) dataset
Target variable: length_of_stay
Main Feature Categories
Category	Example Columns	Description
Identifiers / Dates	episode_id, visit_date, discharge_date, facility_id	Episode (hospital stay) details
Demographics	gender, readmission_count	Basic patient info
Comorbidities	asthma, depression, pneumonia, malnutrition, substance_dependence, etc.	0/1 flags for chronic or acute conditions
Lab Tests & Vitals	hemoglobin, hematocrit, sodium, glucose, blood_urea_nitrogen, creatinine, bmi, pulse, respiration	Clinical measurements that may affect recovery time
________________________________________
ğŸ§¹ Data Preprocessing
All preprocessing steps were performed programmatically in the notebook.
1.	Column Renaming
Shortened and standardized column names for clarity (e.g., eid â†’ episode_id, facid â†’ facility_id, lengthofstay â†’ length_of_stay, etc.).
2.	Datetime Conversion
Converted visit_date and discharge_date into datetime format.
Extracted year, month, day, and weekday from both, then dropped the original columns.
3.	Data Cleaning
o	Replaced '5+' in readmission_count with numeric 5.
o	Converted readmission_count to integer type.
4.	Encoding
o	gender and facility_id encoded using LabelEncoder.
5.	Scaling
o	Applied StandardScaler to all numeric features.
6.	Feature Selection
o	Used Mutual Information and Correlation Analysis to understand which variables influence length_of_stay.
o	Dropped low-importance and redundant columns listed in X_drop_cols.
7.	Multicollinearity Check
o	Used Variance Inflation Factor (VIF) to ensure no strong linear dependencies among predictors.
8.	Train-Test Split
o	80% training and 20% testing (random_state=42).
________________________________________
ğŸ§  Model Development
All models were built using a Pipeline that included the preprocessing transformer and the regression model.
Models Trained
Model	Description
K-Nearest Neighbors (KNN)	Simple, non-parametric baseline model
Decision Tree Regressor	Interpretable tree-based model
Random Forest Regressor	Ensemble of trees reducing variance
AdaBoost Regressor	Sequential ensemble improving weak learners
Gradient Boosting Regressor	Sequential ensemble minimizing bias
XGBoost Regressor	Optimized gradient boosting (fast, accurate)
Linear Regression	Simple baseline for linear relationships
SVR (Support Vector Regressor)	Tried for non-linear patterns (not finalized in output)
________________________________________
ğŸ“ˆ Model Evaluation
Models were evaluated using RÂ² Score (explained variance) and RMSE (average prediction error).
Model	RÂ²	RMSE	Remarks
KNN	0.7638	1.1386	Strong local baseline
Decision Tree	0.6904	1.3033	Slight overfitting
Random Forest	0.7816	1.0946	Stable, good performance
AdaBoost	0.1384	2.1743	Underperformed on this dataset
Gradient Boost	0.8072	1.0286	Excellent predictive strength
XGBoost	0.8109	1.0187	âœ… Best model overall
Linear Regression	0.7577	1.1530	Solid baseline, interpretable
ğŸ§© Interpretation:
RMSE measures the average error magnitude in the same unit as the target.
Since the target is length of stay (days), an RMSE â‰ˆ 1.02 means the modelâ€™s predictions are, on average, about one day off from the actual stay duration.
________________________________________
ğŸ† Final Model
â€¢	Chosen Model: XGBoost Regressor
â€¢	Saved Pipeline: regression_model_rf.pkl (contains XGBoost model despite filename)
â€¢	Evaluation: RÂ² = 0.8109, RMSE = 1.0187
________________________________________
ğŸ“¦ Tools & Libraries
â€¢	Programming Language: Python
â€¢	Libraries: NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn, Statsmodels, XGBoost
â€¢	Deployment: Streamlit + Hugging Face Spaces
________________________________________
ğŸ“Š Visualizations in Notebook
â€¢	Correlation heatmap between features and target
â€¢	Mutual Information ranking plot
â€¢	VIF table for multicollinearity check
â€¢	Model performance comparison (RÂ² & RMSE)
________________________________________
ğŸ’¡ Insights & Business Impact
â€¢	Features like readmission_count, certain lab results, and comorbidities influence stay length.
â€¢	Ensemble models (especially XGBoost) handle these complex relationships best.
â€¢	The project demonstrates how ML can support hospital resource optimization, capacity forecasting, and better patient flow management.
________________________________________
ğŸš€ Deployment
A simple Streamlit web app was created and deployed on Hugging Face Spaces.
It allows users to input patient data and predict the estimated hospital stay duration.
ğŸ”— Live Demo: Patient Stay Estimator â€“ Hugging Face
________________________________________
ğŸ“ Repository Structure
â”‚
â”œâ”€â”€ ML_Regression_Model.ipynb      # Main Jupyter notebook (EDA + Modeling)
â”œâ”€â”€ regression_model_rf.pkl        # Saved XGBoost model pipeline
â”œâ”€â”€ app.py                         # Streamlit web app (for Hugging Face)
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # Project documentation
________________________________________
ğŸ§¾ Conclusion
â€¢	Gradient Boosting and XGBoost provided the most reliable predictions.
â€¢	Achieved an RÂ² â‰ˆ 0.81, indicating strong explanatory power for real-world hospital data.
â€¢	RMSE â‰ˆ 1.02 shows high prediction precision.
â€¢	Demonstrates strong data cleaning, feature engineering, and model comparison workflow.
________________________________________
âœ¨ Author
Divya
ğŸ“« LinkedIn Profile
ğŸ’» Data Science & Machine Learning Enthusiast
â€ƒ
ğŸ’¼ LinkedIn Post (Corrected)
ğŸš‘ Project: Patient Stay Estimator â€” Regression Model
Hospitals need accurate estimates of how long patients will stay for better bed management and resource planning. I built a regression model that predicts Length of Stay (LOS, in days) using encounter-level hospital data and clinical variables.
What I did
â€¢	Cleaned and renamed columns, extracted date features, encoded categorical variables, and scaled numeric values
â€¢	Selected features using mutual information and correlation analysis, and checked multicollinearity (VIF)
â€¢	Trained and compared multiple regressors: KNN, Decision Tree, Random Forest, AdaBoost, Gradient Boosting, XGBoost, and Linear Regression
â€¢	Evaluated models using RÂ² and RMSE
Result
â€¢	XGBoost achieved RÂ² = 0.81 and RMSE = 1.02,
meaning the modelâ€™s predictions differ from actual stay length by roughly one day on average
Impact
Helps hospitals forecast patient stays more accurately, optimize bed allocation, and improve discharge planning.
Tech: Python, pandas, scikit-learn, XGBoost, seaborn, matplotlib, Streamlit (deployed on Hugging Face Spaces)
ğŸ‘‰ Try the demo: https://huggingface.co/spaces/divya55/patient_stay_estimator-Regression_Model
#MachineLearning #HealthcareAI #Regression #XGBoost #DataScience
â€ƒ
